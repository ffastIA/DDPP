import os  # Permite acessar arquivos, pastas e configura√ß√µes do computador
import streamlit as st  # Framework para criar aplica√ß√µes web interativas
from langchain_community.document_loaders import PyPDFLoader  # Carrega arquivos PDF
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Divide textos grandes em peda√ßos menores
from langchain_openai import OpenAIEmbeddings  # Converte texto em n√∫meros (vetores)
from langchain_community.vectorstores import FAISS  # Banco de dados para armazenar e buscar vetores
from langchain_openai import ChatOpenAI  # Modelo de linguagem da OpenAI (GPT)
from langchain.chains import RetrievalQA  # Sistema de perguntas e respostas
from langchain.prompts import ChatPromptTemplate  # Template para formatar perguntas ao modelo
from dotenv import load_dotenv  # Carrega senhas do arquivo .env (arquivo secreto)
import tiktoken  # Conta quantas "palavras" (tokens) tem um texto
from langchain.schema import Document
from langchain_community.document_loaders import DirectoryLoader
from langchain_core.chat_history import InMemoryChatMessageHistory
import uuid  # Para gerar IDs √∫nicos de sess√£o

# ============================ INICIA O STREAMLIT =====================================
st.set_page_config(
    page_title="IAgo",  # T√≠tulo que aparece na aba do navegador
    page_icon="üß†",  # √çcone que aparece na aba
    layout="wide",  # Usa toda a largura da tela
    initial_sidebar_state = "collapsed"  # esconde a barra lateral por padr√£o
)
# =====================================================================================

# ============================ CHAVES DO SISTEMA ===============================
deploy = True # False == roda localmente / True == vers√£o para deploy
cria_vector = False  # False == s√≥ carrega a vector store / True == cria a vector store
# ===============================================================================

# Tenta pegar a chave da API da OpenAI de duas formas diferentes
load_dotenv()
if not deploy:
    api_key = os.getenv("OPENAI_API_KEY")  # Do arquivo .env
else:
    api_key = st.secrets["OPENAI_API_KEY"]  # Do Streamlit secrets (para deploy)

# Verifica se conseguiu pegar a chave da API
if not api_key:
    raise ValueError("A vari√°vel de ambiente 'OPENAI_API_KEY' n√£o foi encontrada no seu arquivo .env.")

# -----------------------------------------------------------------------------
# 1. DEFINI√á√ïES GERAIS
# -----------------------------------------------------------------------------

# Define a chave API como vari√°vel de ambiente
os.environ["OPENAI_API_KEY"] = api_key

# Define o modelo de GPT
modelo = 'gpt-3.5-turbo-0125'  # Qual modelo do ChatGPT usar

# Cria uma inst√¢ncia do modelo de chat da OpenAI
chat_instance = ChatOpenAI(model=modelo)

# Define o modelo de embedding
embeddings_model = OpenAIEmbeddings()  # Modelo para converter texto em n√∫meros

# Define o local onde o vector store persistir√°
diretorio_vectorstore_faiss = 'vectorstore_faiss'  # Onde salvar o banco vetorial

# Define o diret√≥rio onde os arquivos para gerar o vector store est√£o localizados
caminho_arquivo = 'docs'  # Caminho dos arquivos para analisar

# Define a quantidade m√°xima de documentos retornados pela fun√ß√£o retriever()
qtd_retriever = 4

# -----------------------------------------------------------------------------
# 1.5. CONFIGURA√á√ÉO DE MEM√ìRIA CONVERSACIONAL
# -----------------------------------------------------------------------------

#Dicion√°rio global para armazenar hist√≥ricos por sess√£o
if 'memory_store' not in st.session_state:
    st.session_state.memory_store = {}


def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """
    Fun√ß√£o que usa st.session_state para hist√≥rico
    """
    if session_id not in st.session_state.memory_store:
        st.session_state.memory_store[session_id] = InMemoryChatMessageHistory()
        print(f"Criada nova sess√£o {session_id}")  # Debug
    return st.session_state.memory_store[session_id]


def clear_session_history(session_id: str):
    """Limpa o hist√≥rico de uma sess√£o espec√≠fica"""
    if session_id in st.session_state.memory_store:
        st.session_state.memory_store[session_id].clear()
        print(f"Limpeza da sess√£o {session_id}")  # Debug


def get_active_sessions():
    """Retorna lista de sess√µes ativas do session_state"""
    return list(st.session_state.memory_store.keys())


def get_session_message_count(session_id: str):
    """Retorna o n√∫mero de mensagens usando session_state"""
    if session_id in st.session_state.memory_store:
        return len(st.session_state.memory_store[session_id].messages)
    return 0


#Fun√ß√£o para testar e for√ßar salvamento
def force_save_test_message(session_id: str):
    """For√ßa o salvamento de uma mensagem de teste"""
    history = get_session_history(session_id)
    history.add_user_message("TESTE: Mensagem de teste")
    history.add_ai_message("TESTE: Resposta de teste")
    return len(history.messages)


# -----------------------------------------------------------------------------
# 2. PROMPTS COM MEM√ìRIA CONVERSACIONAL
# -----------------------------------------------------------------------------

# Template para pergunta com contexto conversacional
prompt_inicial = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© o IAgo, colaborador especializado no projeto DESPERTAR DIGITAL.
    Sua expertise √© esclarecer d√∫vidas sobre este projeto de forma conversacional e personalizada.

    DIRETRIZES IMPORTANTES:
    - Use o contexto fornecido do documento para responder √†s perguntas
    - CONSIDERE o hist√≥rico da conversa para dar respostas mais personalizadas
    - Refira-se a perguntas anteriores quando relevante ("Como mencionei antes...", "Complementando sua pergunta anterior...")
    - Voc√™ pode buscar dados complementares na internet, mas SEMPRE forne√ßa as fontes
    - Se n√£o souber a resposta, seja transparente e diga que n√£o sabe
    - NUNCA invente informa√ß√µes
    - Mantenha um tom conversacional e amig√°vel
    - Adapte suas respostas baseado no n√≠vel de conhecimento demonstrado pelo usu√°rio
    - Se for a primeira pergunta da conversa, apresente-se brevemente

    CONTEXTO DO DOCUMENTO:
    {context}"""),

    ("placeholder", "{chat_history}"),  # Aqui ser√° inserido o hist√≥rico

    ("human", "{question}"),
])

# Template para tradu√ß√£o com contexto conversacional
translation_prompt = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© um tradutor especializado em textos t√©cnicos sobre aquicultura e biometria.

    INSTRU√á√ïES:
    - Traduza mantendo precis√£o t√©cnica
    - Preserve formata√ß√£o e termos t√©cnicos espec√≠ficos
    - Considere o contexto conversacional se houver refer√™ncias anteriores
    - Mantenha naturalidade em ingl√™s
    - Se houver refer√™ncias a conversas anteriores, traduza-as adequadamente"""),

    ("placeholder", "{chat_history}"),  # Hist√≥rico para contexto de tradu√ß√£o

    ("human", "Traduza este texto: {text}"),
])


# -----------------------------------------------------------------------------
# 3. FUN√á√ïES
# -----------------------------------------------------------------------------

def cria_vector_store_faiss(chunk: list[Document], diretorio_vectorestore_faiss: str) -> FAISS:
    vector_store = FAISS.from_documents(chunk, embeddings_model)
    vector_store.save_local(diretorio_vectorestore_faiss)
    return vector_store


def carrega_vector_store_faiss(diretorio_vectorestore_faiss, embedding_model):
    vector_store = FAISS.load_local(
        diretorio_vectorestore_faiss,
        embedding_model,
        allow_dangerous_deserialization=True
    )
    return vector_store


def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens


def cria_chunks(caminho: str, chunk_size: int, chunk_overlap: int) -> list:
    print(f"Carregando documentos do PDF: {caminho_arquivo}")

    loader = DirectoryLoader(
        path=caminho,
        glob="*.pdf",
        loader_cls=PyPDFLoader,
        recursive=False
    )

    documentos = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=num_tokens_from_string,
        separators=['&', '\n\n', '.', ' '],
        add_start_index=True
    )

    chunk = text_splitter.split_documents(documentos)
    print(f"Texto original dividido em {len(chunk)} chunks.\n")
    return chunk


def format_docs(docs: list[Document]):
    return "\n\n".join(doc.page_content for doc in docs)


def retriever(pergunta: str, n: int):
    resultado = vectorstore.as_retriever(search_type='mmr', search_kwargs={"k": n})
    documentos_retornados = resultado.get_relevant_documents(pergunta)
    return documentos_retornados


# -----------------------------------------------------------------------------
# 4. VECTOR STORE
# -----------------------------------------------------------------------------

if cria_vector:
    chunks = cria_chunks(caminho_arquivo, 500, 50)
    vectorstore = cria_vector_store_faiss(chunks, diretorio_vectorstore_faiss)
else:
    vectorstore = carrega_vector_store_faiss(diretorio_vectorstore_faiss, embeddings_model)

# -----------------------------------------------------------------------------
# 6. T√çTULO E DESCRI√á√ÉO DA P√ÅGINA PELO STREAMLIT
# -----------------------------------------------------------------------------

st.title("üß† PROJETO DESPERTAR DIGITAR")
st.markdown("""
Esta aplica√ß√£o permite que voc√™ consulte os projetos Despertar Digital usando Intelig√™ncia Artificial **com mem√≥ria conversacional**.
Fa√ßa perguntas sobre seus objetivos, indicadores e benef√≠cios! 
A IA se lembrar√° das suas perguntas anteriores para dar respostas mais personalizadas.
""")


# ================================== PIPELINE COM MEM√ìRIA  =======================================
# VERS√ÉO FINAL - COM HIST√ìRICO FOR√áADO
# ================================================================================================

#Fun√ß√£o auxiliar para buscar contexto
def get_context_for_question(question: str) -> str:
    """Busca documentos relevantes e formata como contexto"""
    docs = retriever(question, qtd_retriever)
    return format_docs(docs)


#Fun√ß√£o que inclui hist√≥rico manualmente no prompt
def create_rag_response_with_history(question: str, session_id: str) -> str:
    """
    Fun√ß√£o que cria resposta RAG COM HIST√ìRICO MANUAL
    """
    # Busca o contexto dos documentos
    context = get_context_for_question(question)

    #Pega o hist√≥rico manualmente
    history = get_session_history(session_id)

    # Formata o hist√≥rico para incluir no prompt
    chat_history_text = ""
    if history.messages:
        chat_history_text = "\n\nHIST√ìRICO DA CONVERSA:\n"
        for msg in history.messages[-10:]:  # √öltimas 10 mensagens
            if msg.type == "human":
                chat_history_text += f"üë§ Usu√°rio: {msg.content}\n"
            else:
                chat_history_text += f"ü§ñ Assistente: {msg.content}\n"
        chat_history_text += "\n"

    # Monta o prompt completo com hist√≥rico
    full_prompt = f"""Voc√™ √© o IAgo, colaborador especializado no projeto DESPERTAR DIGITAL da SETEC e do Instituto Idear.
        Sua expertise √© esclarecer d√∫vidas sobre este projeto de forma conversacional e personalizada.
        
        DIRETRIZES IMPORTANTES:
        - Use o contexto fornecido do documento para responder √†s perguntas
        - CONSIDERE o hist√≥rico da conversa para dar respostas mais personalizadas
        - Refira-se a perguntas anteriores quando relevante ("Como mencionei antes...", "Complementando sua pergunta anterior...")
        - Voc√™ pode buscar dados complementares na internet, mas SEMPRE forne√ßa as fontes
        - Se n√£o souber a resposta, seja transparente e diga que n√£o sabe
        - NUNCA invente informa√ß√µes
        - Mantenha um tom conversacional e amig√°vel
        - Adapte suas respostas baseado no n√≠vel de conhecimento demonstrado pelo usu√°rio
        - Se for a primeira pergunta da conversa, apresente-se brevemente
        
        CONTEXTO DO DOCUMENTO:
        {context}
        
        {chat_history_text}
        
        PERGUNTA ATUAL: {question}
        
        Responda considerando todo o contexto e hist√≥rico acima:"""

    # Executa o prompt
    response = chat_instance.invoke(full_prompt)
    return response.content


#Fun√ß√£o de tradu√ß√£o com hist√≥rico manual
def create_translation_with_history(text: str, session_id: str) -> str:
    """
    Fun√ß√£o que traduz COM HIST√ìRICO MANUAL
    """
    #Pega o hist√≥rico manualmente
    history = get_session_history(session_id)

    # Formata o hist√≥rico para contexto de tradu√ß√£o
    chat_history_text = ""
    if history.messages:
        chat_history_text = "\n\nCONTEXTO DA CONVERSA (para refer√™ncia):\n"
        for msg in history.messages[-4:]:  # √öltimas 4 mensagens
            if msg.type == "human":
                chat_history_text += f"Usu√°rio perguntou: {msg.content[:100]}...\n"
            else:
                chat_history_text += f"Assistente respondeu: {msg.content[:100]}...\n"

    # Monta o prompt de tradu√ß√£o
    translation_prompt_text = f"""Voc√™ √© um tradutor especializado em textos t√©cnicos sobre projetos sociais.
        
        INSTRU√á√ïES:
        - Traduza para a lingua inglesa mantendo precis√£o t√©cnica
        - Preserve formata√ß√£o e termos t√©cnicos espec√≠ficos
        - Considere o contexto conversacional se houver refer√™ncias anteriores
        - Mantenha naturalidade em ingl√™s
        - Se houver refer√™ncias a conversas anteriores, traduza-as adequadamente
        
        {chat_history_text}
        
        TEXTO PARA TRADUZIR:
        {text}
        
        Traduza para ingl√™s:"""

    # Executa a tradu√ß√£o
    response = chat_instance.invoke(translation_prompt_text)
    return response.content


#Fun√ß√£o principal COMPLETAMENTE REESCRITA
def qa_chain_complete_with_memory(query: str, session_id: str):
    """Fun√ß√£o FINAL com hist√≥rico manual garantido"""

    try:
        print(f"Processando pergunta: {query}")
        print(f"Session ID: {session_id}")

        #Pega o hist√≥rico ANTES de processar
        history = get_session_history(session_id)
        print(f"Mensagens no hist√≥rico ANTES: {len(history.messages)}")

        #Mostra o conte√∫do do hist√≥rico para debug
        if history.messages:
            print("√öltimas mensagens do hist√≥rico:")
            for i, msg in enumerate(history.messages[-2:]):
                print(f"  {i + 1}. {msg.type}: {msg.content[:50]}...")

        # Primeiro: gera resposta COM HIST√ìRICO MANUAL
        resposta_original = create_rag_response_with_history(query, session_id)

        # Segundo: traduz COM HIST√ìRICO MANUAL
        resposta_traduzida = create_translation_with_history(resposta_original, session_id)

        # üîß CORRE√á√ÉO: SALVA AS MENSAGENS NO HIST√ìRICO
        history.add_user_message(query)
        history.add_ai_message(resposta_original)

        print(f"Mensagens no hist√≥rico DEPOIS: {len(history.messages)}")

        # Terceiro: busca documentos fonte
        source_docs = retriever(query, qtd_retriever)

        print(f"Resposta gerada com sucesso")

        return {
            "result": resposta_original,
            "translated": resposta_traduzida,
            "source_documents": source_docs,
            "session_id": session_id
        }

    except Exception as e:
        print(f"Erro detalhado: {type(e).__name__}: {str(e)}")
        st.error(f"Erro ao processar pergunta: {str(e)}")

        #Fallback simples MAS com salvamento
        try:
            st.warning("Tentando processar sem contexto avan√ßado...")

            # Resposta simples
            context = get_context_for_question(query)
            simple_response = chat_instance.invoke(
                f"Baseado no contexto sobre os projetos Despetar Digital , responda de forma amig√°vel: {query}\n\nContexto: {context}"
            ).content

            # Tradu√ß√£o simples
            simple_translation = chat_instance.invoke(
                f"Traduza para ingl√™s mantendo o tom t√©cnico: {simple_response}"
            ).content

            # üîß CORRE√á√ÉO: Salva no hist√≥rico mesmo no fallback
            history = get_session_history(session_id)
            history.add_user_message(query)
            history.add_ai_message(simple_response)

            print(f"Fallback executado, hist√≥rico tem {len(history.messages)} mensagens")

            return {
                "result": simple_response,
                "translated": simple_translation,
                "source_documents": retriever(query, qtd_retriever),
                "session_id": session_id
            }

        except Exception as fallback_error:
            print(f"Erro no fallback: {str(fallback_error)}")
            return {
                "result": "Desculpe, ocorreu um erro t√©cnico. Tente reformular sua pergunta.",
                "translated": "Sorry, a technical error occurred. Please try rephrasing your question.",
                "source_documents": [],
                "session_id": session_id
            }


# ================================================
# 10. INTERFACE DE CHAT COM MEM√ìRIA
# ================================================

st.markdown("---")
#st.header("üí¨ Converse com o projeto BIA!")
st.header("üí¨ Converse com o consultor virtual ->  IAgo!")

#Inicializa session_id e garante que seja adicionado ao store
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
    #For√ßa a cria√ß√£o da sess√£o no store
    get_session_history(st.session_state.session_id)

# Sidebar para gerenciar sess√µes e mem√≥ria
with st.sidebar:
    st.subheader("üß† Gerenciamento de Mem√≥ria")

    # Mostra ID da sess√£o atual (primeiros 8 caracteres)
    st.text(f"Sess√£o: {st.session_state.session_id[:8]}...")

    #Mostra estat√≠sticas da sess√£o com refresh autom√°tico
    message_count = get_session_message_count(st.session_state.session_id)
    st.metric("Mensagens na mem√≥ria", message_count)

    #Bot√£o para for√ßar atualiza√ß√£o
    if st.button("üîÑ Atualizar Contadores"):
        st.rerun()

    # Bot√£o para limpar mem√≥ria da sess√£o atual
    if st.button("üóëÔ∏è Limpar Mem√≥ria", help="Limpa o hist√≥rico conversacional da IA"):
        clear_session_history(st.session_state.session_id)
        st.session_state.messages = []
        st.success("Mem√≥ria conversacional limpa!")
        st.rerun()

    # Bot√£o para nova sess√£o
    if st.button("üÜï Nova Sess√£o", help="Inicia uma nova conversa independente"):
        st.session_state.session_id = str(uuid.uuid4())
        st.session_state.messages = []
        # üîß CORRE√á√ÉO: For√ßa a cria√ß√£o da nova sess√£o no store
        get_session_history(st.session_state.session_id)
        st.success("Nova sess√£o iniciada!")
        st.rerun()

    #Mostra n√∫mero de sess√µes ativas
    active_sessions = get_active_sessions()
    st.info(f"Sess√µes ativas: {len(active_sessions)}")

    #Debug das sess√µes MELHORADO
    with st.expander("üîß DEBUG - Detalhes da Mem√≥ria"):
        st.write("**Sess√µes no store:**", active_sessions)
        st.write("**Mensagens no Streamlit:**", len(st.session_state.messages) if "messages" in st.session_state else 0)

        if st.session_state.session_id in st.session_state.memory_store:
            history = st.session_state.memory_store[st.session_state.session_id]
            st.write(f"**Mensagens no LangChain:** {len(history.messages)}")

            #Mostra as √∫ltimas mensagens da mem√≥ria
            if history.messages:
                st.write("**√öltimas mensagens na mem√≥ria:**")
                for i, msg in enumerate(history.messages[-4:]):  # √öltimas 4 mensagens
                    role = "üë§ User" if msg.type == "human" else "ü§ñ AI"
                    content = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
                    st.text(f"{role}: {content}")
        else:
            st.write("**Sess√£o n√£o encontrada no store!**")

    #Bot√£o para testar mem√≥ria MELHORADO
    if st.button("üß™ Testar Mem√≥ria"):
        history = get_session_history(st.session_state.session_id)
        st.write(f"**Teste Atual:** {len(history.messages)} mensagens encontradas")

        if history.messages:
            st.write("**Conte√∫do das mensagens:**")
            for i, msg in enumerate(history.messages):
                role = "üë§ User" if msg.type == "human" else "ü§ñ AI"
                content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                st.text(f"{i + 1}. {role}: {content}")
        else:
            st.write("**Nenhuma mensagem encontrada!**")

        # Bot√£o para for√ßar teste
        if st.button("üîß For√ßar Teste", key="force_test"):
            count = force_save_test_message(st.session_state.session_id)
            st.success(f"Mensagem de teste adicionada! Total: {count}")
            st.rerun()

    # Informa√ß√µes sobre mem√≥ria
    with st.expander("‚ÑπÔ∏è Sobre a Mem√≥ria"):
        st.markdown("""
        **Como funciona:**
        - A IA se lembra das perguntas anteriores
        - Respostas s√£o contextualizadas com a conversa
        - Cada sess√£o mant√©m seu pr√≥prio hist√≥rico
        - Use "Limpar Mem√≥ria" para recome√ßar
        - Use "Nova Sess√£o" para conversa independente

        **üîß Debug:**
        - Use "Atualizar Contadores" se os n√∫meros n√£o baterem
        - Use "Testar Mem√≥ria" para ver o conte√∫do
        - Use "For√ßar Teste" para adicionar mensagens de teste
        """)

# Inicializa o hist√≥rico de mensagens se n√£o existir
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostra o hist√≥rico de mensagens
for msg in st.session_state.messages:
    if msg["role"] == "user":
        with st.chat_message("user", avatar="üë§"):
            st.markdown(msg["content"])
    else:
        with st.chat_message("assistant", avatar="ü§ñ"):
            tab1, tab2 = st.tabs(["üáßüá∑ Portugu√™s", "üá∫üá∏ SOBRAL"])

            with tab1:
                st.markdown(msg["content"])

            with tab2:
                if "translated" in msg:
                    st.markdown(msg["translated"])
                else:
                    st.info("Tradu√ß√£o n√£o dispon√≠vel para mensagens anteriores")

            if "sources" in msg and msg["sources"]:
                with st.expander("üìö Fontes"):
                    for i, source in enumerate(msg["sources"], 1):
                        st.markdown(f"**Fonte {i}:**")
                        st.markdown(source.page_content[:200] + "...")
                        st.markdown(f"*P√°gina: {source.metadata.get('page', 'N/A')}*")
                        st.markdown("---")

# Campo de entrada para nova pergunta
user_input = st.chat_input(
    "Digite sua pergunta sobre o documento... (A IA se lembra das perguntas anteriores)",
    key="user_input_field"
)

# üîß CORRE√á√ÉO: Processa nova pergunta COM MEM√ìRIA MANUAL
if user_input:
    # Adiciona pergunta do usu√°rio ao hist√≥rico do Streamlit
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })

    # Mostra a pergunta do usu√°rio
    with st.chat_message("user", avatar="üë§"):
        st.markdown(user_input)

    # Processa com mem√≥ria conversacional
    with st.chat_message("assistant", avatar="ü§ñ"):
        with st.spinner("üß† Analisando com contexto da conversa..."):
            # üîß CORRE√á√ÉO: USA A FUN√á√ÉO COM HIST√ìRICO MANUAL FOR√áADO
            result = qa_chain_complete_with_memory(
                user_input,
                st.session_state.session_id
            )

            answer = result["result"]
            translated = result["translated"]
            sources = result.get("source_documents", [])

        # Mostra as respostas em abas
        tab1, tab2 = st.tabs(["üáßüá∑ Portugu√™s", "üá∫üá∏ SOBRAL"])

        with tab1:
            st.markdown(answer)

        with tab2:
            st.markdown(translated)

        # Mostra as fontes
        if sources:
            with st.expander("üìö Fontes"):
                for i, source in enumerate(sources, 1):
                    st.markdown(f"**Fonte {i}:**")
                    st.markdown(source.page_content[:200] + "...")
                    st.markdown(f"*P√°gina: {source.metadata.get('page', 'N/A')}*")
                    st.markdown("---")

        # Adiciona resposta ao hist√≥rico do Streamlit
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "translated": translated,
            "sources": sources
        })
