import os  # Permite acessar arquivos, pastas e configuraÃ§Ãµes do computador
import streamlit as st  # Framework para criar aplicaÃ§Ãµes web interativas
from langchain_community.document_loaders import PyPDFLoader  # Carrega arquivos PDF
from langchain.text_splitter import RecursiveCharacterTextSplitter  # Divide textos grandes em pedaÃ§os menores
from langchain_openai import OpenAIEmbeddings  # Converte texto em nÃºmeros (vetores)
from langchain_community.vectorstores import FAISS  # Banco de dados para armazenar e buscar vetores
from langchain_openai import ChatOpenAI  # Modelo de linguagem da OpenAI (GPT)
from langchain.chains import RetrievalQA  # Sistema de perguntas e respostas
from langchain.prompts import ChatPromptTemplate  # Template para formatar perguntas ao modelo
from dotenv import load_dotenv  # Carrega senhas do arquivo .env (arquivo secreto)
import tiktoken  # Conta quantas "palavras" (tokens) tem um texto
from langchain.schema import Document
from langchain_community.document_loaders import DirectoryLoader
from langchain_core.chat_history import InMemoryChatMessageHistory
import uuid  # Para gerar IDs Ãºnicos de sessÃ£o

# ============================ INICIA O STREAMLIT =====================================
st.set_page_config(
    page_title="IAgo",  # TÃ­tulo que aparece na aba do navegador
    page_icon="ğŸ§ ",  # Ãcone que aparece na aba
    layout="wide",  # Usa toda a largura da tela
    initial_sidebar_state = "collapsed"  # esconde a barra lateral por padrÃ£o
)
# =====================================================================================

# ============================ CHAVES DO SISTEMA ===============================
deploy = True # False == roda localmente / True == versÃ£o para deploy
cria_vector = False  # False == sÃ³ carrega a vector store / True == cria a vector store
# ===============================================================================

# Tenta pegar a chave da API da OpenAI de duas formas diferentes
load_dotenv()
if not deploy:
    api_key = os.getenv("OPENAI_API_KEY")  # Do arquivo .env
else:
    api_key = st.secrets["OPENAI_API_KEY"]  # Do Streamlit secrets (para deploy)

# Verifica se conseguiu pegar a chave da API
if not api_key:
    raise ValueError("A variÃ¡vel de ambiente 'OPENAI_API_KEY' nÃ£o foi encontrada no seu arquivo .env.")

# -----------------------------------------------------------------------------
# 1. DEFINIÃ‡Ã•ES GERAIS
# -----------------------------------------------------------------------------

# Define a chave API como variÃ¡vel de ambiente
os.environ["OPENAI_API_KEY"] = api_key

# Define o modelo de GPT
modelo = 'gpt-3.5-turbo-0125'  # Qual modelo do ChatGPT usar

# Cria uma instÃ¢ncia do modelo de chat da OpenAI
chat_instance = ChatOpenAI(model=modelo)

# Define o modelo de embedding
embeddings_model = OpenAIEmbeddings()  # Modelo para converter texto em nÃºmeros

# Define o local onde o vector store persistirÃ¡
diretorio_vectorstore_faiss = 'vectorstore_faiss'  # Onde salvar o banco vetorial

# Define o diretÃ³rio onde os arquivos para gerar o vector store estÃ£o localizados
caminho_arquivo = 'docs'  # Caminho dos arquivos para analisar

# Define a quantidade mÃ¡xima de documentos retornados pela funÃ§Ã£o retriever()
qtd_retriever = 4

# -----------------------------------------------------------------------------
# 1.5. CONFIGURAÃ‡ÃƒO DE MEMÃ“RIA CONVERSACIONAL
# -----------------------------------------------------------------------------

#DicionÃ¡rio global para armazenar histÃ³ricos por sessÃ£o
if 'memory_store' not in st.session_state:
    st.session_state.memory_store = {}


def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """
    FunÃ§Ã£o que usa st.session_state para histÃ³rico
    """
    if session_id not in st.session_state.memory_store:
        st.session_state.memory_store[session_id] = InMemoryChatMessageHistory()
        print(f"Criada nova sessÃ£o {session_id}")  # Debug
    return st.session_state.memory_store[session_id]


def clear_session_history(session_id: str):
    """Limpa o histÃ³rico de uma sessÃ£o especÃ­fica"""
    if session_id in st.session_state.memory_store:
        st.session_state.memory_store[session_id].clear()
        print(f"Limpeza da sessÃ£o {session_id}")  # Debug


def get_active_sessions():
    """Retorna lista de sessÃµes ativas do session_state"""
    return list(st.session_state.memory_store.keys())


def get_session_message_count(session_id: str):
    """Retorna o nÃºmero de mensagens usando session_state"""
    if session_id in st.session_state.memory_store:
        return len(st.session_state.memory_store[session_id].messages)
    return 0


#FunÃ§Ã£o para testar e forÃ§ar salvamento
def force_save_test_message(session_id: str):
    """ForÃ§a o salvamento de uma mensagem de teste"""
    history = get_session_history(session_id)
    history.add_user_message("TESTE: Mensagem de teste")
    history.add_ai_message("TESTE: Resposta de teste")
    return len(history.messages)


# -----------------------------------------------------------------------------
# 2. PROMPTS COM MEMÃ“RIA CONVERSACIONAL
# -----------------------------------------------------------------------------

# Template para pergunta com contexto conversacional
prompt_inicial = ChatPromptTemplate.from_messages([
    ("system", """VocÃª Ã© o IAgo, colaborador especializado no projeto DESPERTAR DIGITAL.
    Sua expertise Ã© esclarecer dÃºvidas sobre este projeto de forma conversacional e personalizada.

    DIRETRIZES IMPORTANTES:
    - Use o contexto fornecido do documento para responder Ã s perguntas
    - CONSIDERE o histÃ³rico da conversa para dar respostas mais personalizadas
    - Refira-se a perguntas anteriores quando relevante ("Como mencionei antes...", "Complementando sua pergunta anterior...")
    - VocÃª pode buscar dados complementares na internet, mas SEMPRE forneÃ§a as fontes
    - Se nÃ£o souber a resposta, seja transparente e diga que nÃ£o sabe
    - NUNCA invente informaÃ§Ãµes
    - Mantenha um tom conversacional e amigÃ¡vel
    - Adapte suas respostas baseado no nÃ­vel de conhecimento demonstrado pelo usuÃ¡rio
    - Se for a primeira pergunta da conversa, apresente-se brevemente

    CONTEXTO DO DOCUMENTO:
    {context}"""),

    ("placeholder", "{chat_history}"),  # Aqui serÃ¡ inserido o histÃ³rico

    ("human", "{question}"),
])

# Template para traduÃ§Ã£o com contexto conversacional
translation_prompt = ChatPromptTemplate.from_messages([
    ("system", """VocÃª Ã© um tradutor especializado em textos tÃ©cnicos sobre aquicultura e biometria.

    INSTRUÃ‡Ã•ES:
    - Traduza mantendo precisÃ£o tÃ©cnica
    - Preserve formataÃ§Ã£o e termos tÃ©cnicos especÃ­ficos
    - Considere o contexto conversacional se houver referÃªncias anteriores
    - Mantenha naturalidade em inglÃªs
    - Se houver referÃªncias a conversas anteriores, traduza-as adequadamente"""),

    ("placeholder", "{chat_history}"),  # HistÃ³rico para contexto de traduÃ§Ã£o

    ("human", "Traduza este texto: {text}"),
])


# -----------------------------------------------------------------------------
# 3. FUNÃ‡Ã•ES
# -----------------------------------------------------------------------------

def cria_vector_store_faiss(chunk: list[Document], diretorio_vectorestore_faiss: str) -> FAISS:
    vector_store = FAISS.from_documents(chunk, embeddings_model)
    vector_store.save_local(diretorio_vectorestore_faiss)
    return vector_store


def carrega_vector_store_faiss(diretorio_vectorestore_faiss, embedding_model):
    vector_store = FAISS.load_local(
        diretorio_vectorestore_faiss,
        embedding_model,
        allow_dangerous_deserialization=True
    )
    return vector_store


def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens


def cria_chunks(caminho: str, chunk_size: int, chunk_overlap: int) -> list:
    print(f"Carregando documentos do PDF: {caminho_arquivo}")

    loader = DirectoryLoader(
        path=caminho,
        glob="*.pdf",
        loader_cls=PyPDFLoader,
        recursive=False
    )

    documentos = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=num_tokens_from_string,
        separators=['&', '\n\n', '.', ' '],
        add_start_index=True
    )

    chunk = text_splitter.split_documents(documentos)
    print(f"Texto original dividido em {len(chunk)} chunks.\n")
    return chunk


def format_docs(docs: list[Document]):
    return "\n\n".join(doc.page_content for doc in docs)


def retriever(pergunta: str, n: int):
    resultado = vectorstore.as_retriever(search_type='mmr', search_kwargs={"k": n})
    documentos_retornados = resultado.get_relevant_documents(pergunta)
    return documentos_retornados


# -----------------------------------------------------------------------------
# 4. VECTOR STORE
# -----------------------------------------------------------------------------

if cria_vector:
    chunks = cria_chunks(caminho_arquivo, 500, 50)
    vectorstore = cria_vector_store_faiss(chunks, diretorio_vectorstore_faiss)
else:
    vectorstore = carrega_vector_store_faiss(diretorio_vectorstore_faiss, embeddings_model)

# -----------------------------------------------------------------------------
# 6. TÃTULO E DESCRIÃ‡ÃƒO DA PÃGINA PELO STREAMLIT
# -----------------------------------------------------------------------------

st.title("ğŸ§  PROJETO DESPERTAR DIGITAR")
st.markdown("""
Esta aplicaÃ§Ã£o permite que vocÃª consulte os projetos Despertar Digital usando InteligÃªncia Artificial **com memÃ³ria conversacional**.
FaÃ§a perguntas sobre seus objetivos, indicadores e benefÃ­cios! 
A IA se lembrarÃ¡ das suas perguntas anteriores para dar respostas mais personalizadas.
""")


# ================================== PIPELINE COM MEMÃ“RIA  =======================================
# VERSÃƒO FINAL - COM HISTÃ“RICO FORÃ‡ADO
# ================================================================================================

#FunÃ§Ã£o auxiliar para buscar contexto
def get_context_for_question(question: str) -> str:
    """Busca documentos relevantes e formata como contexto"""
    docs = retriever(question, qtd_retriever)
    return format_docs(docs)


#FunÃ§Ã£o que inclui histÃ³rico manualmente no prompt
def create_rag_response_with_history(question: str, session_id: str) -> str:
    """
    FunÃ§Ã£o que cria resposta RAG COM HISTÃ“RICO MANUAL
    """
    # Busca o contexto dos documentos
    context = get_context_for_question(question)

    #Pega o histÃ³rico manualmente
    history = get_session_history(session_id)

    # Formata o histÃ³rico para incluir no prompt
    chat_history_text = ""
    if history.messages:
        chat_history_text = "\n\nHISTÃ“RICO DA CONVERSA:\n"
        for msg in history.messages[-10:]:  # Ãšltimas 10 mensagens
            if msg.type == "human":
                chat_history_text += f"ğŸ‘¤ UsuÃ¡rio: {msg.content}\n"
            else:
                chat_history_text += f"ğŸ¤– Assistente: {msg.content}\n"
        chat_history_text += "\n"

    # Monta o prompt completo com histÃ³rico
    full_prompt = f"""VocÃª Ã© o IAgo, colaborador especializado no projeto DESPERTAR DIGITAL da SETEC e do Instituto Idear.
        Sua expertise Ã© esclarecer dÃºvidas sobre este projeto de forma conversacional e personalizada.
        
        DIRETRIZES IMPORTANTES:
        - Use o contexto fornecido do documento para responder Ã s perguntas
        - CONSIDERE o histÃ³rico da conversa para dar respostas mais personalizadas
        - Refira-se a perguntas anteriores quando relevante ("Como mencionei antes...", "Complementando sua pergunta anterior...")
        - VocÃª pode buscar dados complementares na internet, mas SEMPRE forneÃ§a as fontes
        - Se nÃ£o souber a resposta, seja transparente e diga que nÃ£o sabe
        - NUNCA invente informaÃ§Ãµes
        - Mantenha um tom conversacional e amigÃ¡vel
        - Adapte suas respostas baseado no nÃ­vel de conhecimento demonstrado pelo usuÃ¡rio
        - Se for a primeira pergunta da conversa, apresente-se brevemente
        
        CONTEXTO DO DOCUMENTO:
        {context}
        
        {chat_history_text}
        
        PERGUNTA ATUAL: {question}
        
        Responda considerando todo o contexto e histÃ³rico acima:"""

    # Executa o prompt
    response = chat_instance.invoke(full_prompt)
    return response.content


#FunÃ§Ã£o de traduÃ§Ã£o com histÃ³rico manual
def create_translation_with_history(text: str, session_id: str) -> str:
    """
    FunÃ§Ã£o que traduz COM HISTÃ“RICO MANUAL
    """
    #Pega o histÃ³rico manualmente
    history = get_session_history(session_id)

    # Formata o histÃ³rico para contexto de traduÃ§Ã£o
    chat_history_text = ""
    if history.messages:
        chat_history_text = "\n\nCONTEXTO DA CONVERSA (para referÃªncia):\n"
        for msg in history.messages[-4:]:  # Ãšltimas 4 mensagens
            if msg.type == "human":
                chat_history_text += f"UsuÃ¡rio perguntou: {msg.content[:100]}...\n"
            else:
                chat_history_text += f"Assistente respondeu: {msg.content[:100]}...\n"

    # Monta o prompt de traduÃ§Ã£o
    translation_prompt_text = f"""VocÃª Ã© um tradutor especializado em textos tÃ©cnicos sobre projetos sociais.
        
        INSTRUÃ‡Ã•ES:
        - Traduza para a lingua inglesa mantendo precisÃ£o tÃ©cnica
        - Preserve formataÃ§Ã£o e termos tÃ©cnicos especÃ­ficos
        - Considere o contexto conversacional se houver referÃªncias anteriores
        - Mantenha naturalidade em inglÃªs
        - Se houver referÃªncias a conversas anteriores, traduza-as adequadamente
        
        {chat_history_text}
        
        TEXTO PARA TRADUZIR:
        {text}
        
        Traduza para inglÃªs:"""

    # Executa a traduÃ§Ã£o
    response = chat_instance.invoke(translation_prompt_text)
    return response.content


#FunÃ§Ã£o principal COMPLETAMENTE REESCRITA
def qa_chain_complete_with_memory(query: str, session_id: str):
    """FunÃ§Ã£o FINAL com histÃ³rico manual garantido"""

    try:
        print(f"Processando pergunta: {query}")
        print(f"Session ID: {session_id}")

        #Pega o histÃ³rico ANTES de processar
        history = get_session_history(session_id)
        print(f"Mensagens no histÃ³rico ANTES: {len(history.messages)}")

        #Mostra o conteÃºdo do histÃ³rico para debug
        if history.messages:
            print("Ãšltimas mensagens do histÃ³rico:")
            for i, msg in enumerate(history.messages[-2:]):
                print(f"  {i + 1}. {msg.type}: {msg.content[:50]}...")

        # Primeiro: gera resposta COM HISTÃ“RICO MANUAL
        resposta_original = create_rag_response_with_history(query, session_id)

        # Segundo: traduz COM HISTÃ“RICO MANUAL
        resposta_traduzida = create_translation_with_history(resposta_original, session_id)

        # ğŸ”§ CORREÃ‡ÃƒO: SALVA AS MENSAGENS NO HISTÃ“RICO
        history.add_user_message(query)
        history.add_ai_message(resposta_original)

        print(f"Mensagens no histÃ³rico DEPOIS: {len(history.messages)}")

        # Terceiro: busca documentos fonte
        source_docs = retriever(query, qtd_retriever)

        print(f"Resposta gerada com sucesso")

        return {
            "result": resposta_original,
            "translated": resposta_traduzida,
            "source_documents": source_docs,
            "session_id": session_id
        }

    except Exception as e:
        print(f"Erro detalhado: {type(e).__name__}: {str(e)}")
        st.error(f"Erro ao processar pergunta: {str(e)}")

        #Fallback simples MAS com salvamento
        try:
            st.warning("Tentando processar sem contexto avanÃ§ado...")

            # Resposta simples
            context = get_context_for_question(query)
            simple_response = chat_instance.invoke(
                f"Baseado no contexto sobre os projetos Despetar Digital , responda de forma amigÃ¡vel: {query}\n\nContexto: {context}"
            ).content

            # TraduÃ§Ã£o simples
            simple_translation = chat_instance.invoke(
                f"Traduza para inglÃªs mantendo o tom tÃ©cnico: {simple_response}"
            ).content

            # ğŸ”§ CORREÃ‡ÃƒO: Salva no histÃ³rico mesmo no fallback
            history = get_session_history(session_id)
            history.add_user_message(query)
            history.add_ai_message(simple_response)

            print(f"Fallback executado, histÃ³rico tem {len(history.messages)} mensagens")

            return {
                "result": simple_response,
                "translated": simple_translation,
                "source_documents": retriever(query, qtd_retriever),
                "session_id": session_id
            }

        except Exception as fallback_error:
            print(f"Erro no fallback: {str(fallback_error)}")
            return {
                "result": "Desculpe, ocorreu um erro tÃ©cnico. Tente reformular sua pergunta.",
                "translated": "Sorry, a technical error occurred. Please try rephrasing your question.",
                "source_documents": [],
                "session_id": session_id
            }


# ================================================
# 10. INTERFACE DE CHAT COM MEMÃ“RIA
# ================================================

st.markdown("---")
#st.header("ğŸ’¬ Converse com o projeto BIA!")
st.header("ğŸ’¬ Converse com o consultor virtual ->  IAgo!")

#Inicializa session_id e garante que seja adicionado ao store
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
    #ForÃ§a a criaÃ§Ã£o da sessÃ£o no store
    get_session_history(st.session_state.session_id)

# Sidebar para gerenciar sessÃµes e memÃ³ria
with st.sidebar:
    st.subheader("ğŸ§  Gerenciamento de MemÃ³ria")

    # Mostra ID da sessÃ£o atual (primeiros 8 caracteres)
    st.text(f"SessÃ£o: {st.session_state.session_id[:8]}...")

    #Mostra estatÃ­sticas da sessÃ£o com refresh automÃ¡tico
    message_count = get_session_message_count(st.session_state.session_id)
    st.metric("Mensagens na memÃ³ria", message_count)

    #BotÃ£o para forÃ§ar atualizaÃ§Ã£o
    if st.button("ğŸ”„ Atualizar Contadores"):
        st.rerun()

    # BotÃ£o para limpar memÃ³ria da sessÃ£o atual
    if st.button("ğŸ—‘ï¸ Limpar MemÃ³ria", help="Limpa o histÃ³rico conversacional da IA"):
        clear_session_history(st.session_state.session_id)
        st.session_state.messages = []
        st.success("MemÃ³ria conversacional limpa!")
        st.rerun()

    # BotÃ£o para nova sessÃ£o
    if st.button("ğŸ†• Nova SessÃ£o", help="Inicia uma nova conversa independente"):
        st.session_state.session_id = str(uuid.uuid4())
        st.session_state.messages = []
        # ğŸ”§ CORREÃ‡ÃƒO: ForÃ§a a criaÃ§Ã£o da nova sessÃ£o no store
        get_session_history(st.session_state.session_id)
        st.success("Nova sessÃ£o iniciada!")
        st.rerun()

    #Mostra nÃºmero de sessÃµes ativas
    active_sessions = get_active_sessions()
    st.info(f"SessÃµes ativas: {len(active_sessions)}")

    #Debug das sessÃµes MELHORADO
    with st.expander("ğŸ”§ DEBUG - Detalhes da MemÃ³ria"):
        st.write("**SessÃµes no store:**", active_sessions)
        st.write("**Mensagens no Streamlit:**", len(st.session_state.messages) if "messages" in st.session_state else 0)

        if st.session_state.session_id in st.session_state.memory_store:
            history = st.session_state.memory_store[st.session_state.session_id]
            st.write(f"**Mensagens no LangChain:** {len(history.messages)}")

            #Mostra as Ãºltimas mensagens da memÃ³ria
            if history.messages:
                st.write("**Ãšltimas mensagens na memÃ³ria:**")
                for i, msg in enumerate(history.messages[-4:]):  # Ãšltimas 4 mensagens
                    role = "ğŸ‘¤ User" if msg.type == "human" else "ğŸ¤– AI"
                    content = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
                    st.text(f"{role}: {content}")
        else:
            st.write("**SessÃ£o nÃ£o encontrada no store!**")

    #BotÃ£o para testar memÃ³ria MELHORADO
    if st.button("ğŸ§ª Testar MemÃ³ria"):
        history = get_session_history(st.session_state.session_id)
        st.write(f"**Teste Atual:** {len(history.messages)} mensagens encontradas")

        if history.messages:
            st.write("**ConteÃºdo das mensagens:**")
            for i, msg in enumerate(history.messages):
                role = "ğŸ‘¤ User" if msg.type == "human" else "ğŸ¤– AI"
                content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                st.text(f"{i + 1}. {role}: {content}")
        else:
            st.write("**Nenhuma mensagem encontrada!**")

        # BotÃ£o para forÃ§ar teste
        if st.button("ğŸ”§ ForÃ§ar Teste", key="force_test"):
            count = force_save_test_message(st.session_state.session_id)
            st.success(f"Mensagem de teste adicionada! Total: {count}")
            st.rerun()

    # InformaÃ§Ãµes sobre memÃ³ria
    with st.expander("â„¹ï¸ Sobre a MemÃ³ria"):
        st.markdown("""
        **Como funciona:**
        - A IA se lembra das perguntas anteriores
        - Respostas sÃ£o contextualizadas com a conversa
        - Cada sessÃ£o mantÃ©m seu prÃ³prio histÃ³rico
        - Use "Limpar MemÃ³ria" para recomeÃ§ar
        - Use "Nova SessÃ£o" para conversa independente

        **ğŸ”§ Debug:**
        - Use "Atualizar Contadores" se os nÃºmeros nÃ£o baterem
        - Use "Testar MemÃ³ria" para ver o conteÃºdo
        - Use "ForÃ§ar Teste" para adicionar mensagens de teste
        """)

# Inicializa o histÃ³rico de mensagens se nÃ£o existir
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostra o histÃ³rico de mensagens
for msg in st.session_state.messages:
    if msg["role"] == "user":
        with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(msg["content"])
    else:
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            tab1, tab2 = st.tabs(["ğŸ‡§ğŸ‡· PortuguÃªs", "ğŸ‡ºğŸ‡¸ SOBRAL"])

            with tab1:
                st.markdown(msg["content"])

            with tab2:
                if "translated" in msg:
                    st.markdown(msg["translated"])
                else:
                    st.info("TraduÃ§Ã£o nÃ£o disponÃ­vel para mensagens anteriores")

            if "sources" in msg and msg["sources"]:
                with st.expander("ğŸ“š Fontes"):
                    for i, source in enumerate(msg["sources"], 1):
                        st.markdown(f"**Fonte {i}:**")
                        st.markdown(source.page_content[:200] + "...")
                        st.markdown(f"*PÃ¡gina: {source.metadata.get('page', 'N/A')}*")
                        st.markdown("---")

# Campo de entrada para nova pergunta
user_input = st.chat_input(
    "Digite sua pergunta sobre o documento... (A IA se lembra das perguntas anteriores)",
    key="user_input_field"
)

# ğŸ”§ CORREÃ‡ÃƒO: Processa nova pergunta COM MEMÃ“RIA MANUAL
if user_input:
    # Adiciona pergunta do usuÃ¡rio ao histÃ³rico do Streamlit
    st.session_state.messages.append({
        "role": "user",
        "content": user_input
    })

    # Mostra a pergunta do usuÃ¡rio
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(user_input)

    # Processa com memÃ³ria conversacional
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        with st.spinner("ğŸ§  Analisando com contexto da conversa..."):
            # ğŸ”§ CORREÃ‡ÃƒO: USA A FUNÃ‡ÃƒO COM HISTÃ“RICO MANUAL FORÃ‡ADO
            result = qa_chain_complete_with_memory(
                user_input,
                st.session_state.session_id
            )

            answer = result["result"]
            translated = result["translated"]
            sources = result.get("source_documents", [])

        # Mostra as respostas em abas
        tab1, tab2 = st.tabs(["ğŸ‡§ğŸ‡· PortuguÃªs", "ğŸ‡ºğŸ‡¸ SOBRAL"])

        with tab1:
            st.markdown(answer)

        with tab2:
            st.markdown(translated)

        # Mostra as fontes
        if sources:
            with st.expander("ğŸ“š Fontes"):
                for i, source in enumerate(sources, 1):
                    st.markdown(f"**Fonte {i}:**")
                    st.markdown(source.page_content[:200] + "...")
                    st.markdown(f"*PÃ¡gina: {source.metadata.get('page', 'N/A')}*")
                    st.markdown("---")

        # Adiciona resposta ao histÃ³rico do Streamlit
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "translated": translated,
            "sources": sources
        })
